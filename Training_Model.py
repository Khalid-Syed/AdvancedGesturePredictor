# -*- coding: utf-8 -*-
"""Khalid_DL_Final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hwcWw5DR4VV91HN0yv1ybMoSCQXBdcp5
"""



import tensorflow as tf
import time
import matplotlib.pyplot as plt

import matplotlib.pyplot as plt

# Function to train the model and track loss
def train_model_with_loss_tracking(model, train_dataset, epochs, optimizer, loss_fn):
    epoch_losses = []  # Track loss for each epoch
    for epoch in range(epochs):
        total_loss = 0
        batch_count = 0
        for inputs, labels in train_dataset:
            loss = train_step(inputs, labels, model, optimizer, loss_fn)
            total_loss += loss.numpy()
            batch_count += 1
        avg_loss = total_loss / batch_count
        epoch_losses.append(avg_loss)
        print(f"Epoch {epoch + 1}, Loss: {avg_loss:.4f}")
    return epoch_losses

# Plotting function
def plot_results(epoch_losses, evaluation_results):
    # Unpack evaluation results
    branch1_count = evaluation_results["early_exit1_count"]
    branch2_count = evaluation_results["early_exit2_count"]
    main_exit_count = evaluation_results["main_exit_count"]
    avg_branch1_time = evaluation_results["avg_time_early_exit1"]
    avg_branch2_time = evaluation_results["avg_time_early_exit2"]
    avg_main_time = evaluation_results["avg_time_main_exit"]

    # Create plots
    plt.figure(figsize=(18, 6))

    # Plot 1: Loss over epochs
    plt.subplot(1, 3, 1)
    plt.plot(range(1, len(epoch_losses) + 1), epoch_losses, marker="o", label="Training Loss")
    plt.title("Loss Over Epochs")
    plt.xlabel("Epochs")
    plt.ylabel("Loss")
    plt.legend()

    # Plot 2: Branch exit counts
    plt.subplot(1, 3, 2)
    branches = ["Branch 1", "Branch 2", "Main Exit"]
    counts = [branch1_count, branch2_count, main_exit_count]
    plt.bar(branches, counts, color=["skyblue", "orange", "green"])
    plt.title("Branch Exit Counts")
    plt.ylabel("Count")
    for i, count in enumerate(counts):
        plt.text(i, count + 1, str(count), ha="center", fontsize=12)

    # Plot 3: Average inference times
    plt.subplot(1, 3, 3)
    exits = ["Branch 1", "Branch 2", "Main Exit"]
    times = [avg_branch1_time, avg_branch2_time, avg_main_time]
    plt.bar(exits, times, color=["skyblue", "orange", "green"])
    plt.title("Average Inference Times")
    plt.ylabel("Time (seconds)")
    for i, time in enumerate(times):
        plt.text(i, time + 0.0005, f"{time:.6f}", ha="center", fontsize=10)

    # Show the plots
    plt.tight_layout()
    plt.show()

# Example usage
if __name__ == "__main__":
    # Example dataset
    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
    x_train = x_train[..., None].astype("float32") / 255.0
    x_test = x_test[..., None].astype("float32") / 255.0

    train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(1000).batch(32)
    test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)

    # Initialize model
    branchy_model = BranchyNetwork()
    optimizer = tf.keras.optimizers.Adam()
    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)

    # Train model and track losses
    print("Training the model...")
    losses = train_model_with_loss_tracking(branchy_model, train_dataset, epochs=5, optimizer=optimizer, loss_fn=loss_fn)

    # # Evaluate model
    # evaluation_results = evaluate_with_early_exits(branchy_model, test_dataset, confidence_threshold=0.8)

    # for key, value in evaluation_results.items():
    #     print(f"{key}: {value}")


    # # Plot results
    # plot_results(losses, evaluation_results)

# Evaluate model
evaluation_results = evaluate_with_early_exits(branchy_model, test_dataset, confidence_threshold=0.8)

for key, value in evaluation_results.items():
        print(f"{key}: {value}")

# Plot results
plot_results(losses, evaluation_results)

import tensorflow as tf

# class BaseNetwork(tf.keras.Model):
#     def __init__(self):
#         super().__init__()
#         self.conv1 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu')
#         self.conv2 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu')
#         self.flatten = tf.keras.layers.Flatten()
#         self.fc = tf.keras.layers.Dense(10)  # Main output layer

#     def call(self, inputs):
#         x = self.conv1(inputs)
#         x = self.conv2(x)
#         x = self.flatten(x)
#         return self.fc(x)


class BranchyNetwork(tf.keras.Model):
    def __init__(self):
        super().__init__()
        self.conv1 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu')
        self.conv2 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu')
        self.flatten = tf.keras.layers.Flatten()
        self.fc = tf.keras.layers.Dense(10)  # Main output layer

        # Branch 1
        self.branch1_pool = tf.keras.layers.GlobalAveragePooling2D()
        self.branch1_fc = tf.keras.layers.Dense(10)  # Auxiliary output 1

        # Branch 2
        self.branch2_pool = tf.keras.layers.GlobalAveragePooling2D()
        self.branch2_fc = tf.keras.layers.Dense(10)  # Auxiliary output 2

    def call(self, inputs):
        x = self.conv1(inputs)
        b1 = self.branch1_pool(x)
        b1 = self.branch1_fc(b1)  # Branch 1 output

        x = self.conv2(x)
        b2 = self.branch2_pool(x)
        b2 = self.branch2_fc(b2)  # Branch 2 output

        x = self.flatten(x)
        main_output = self.fc(x)  # Main output

        return b1, b2, main_output

    # def inference(self, inputs, confidence_threshold= 0.8):
    #     x = self.conv1(inputs)
    #     b1 = self.branch1_pool(x)
    #     b1 = self.branch1_fc(b1)  # Branch 1 output
    #     b1_probs = tf.nn.softmax(b1)
    #     if tf.reduce_max(b1_probs) > confidence_threshold:
    #       return tf.argmax(b1_probs, axis=1), 1  # Early exit at Branch 1

    #     x = self.conv2(x)
    #     b2 = self.branch2_pool(x)
    #     b2 = self.branch2_fc(b2)  # Branch 2 output
    #     b2_probs = tf.nn.softmax(b2)
    #     elif tf.reduce_max(b2_probs) > confidence_threshold:
    #       return tf.argmax(b2_probs, axis=1), 2  # Early exit at Branch 2

    #     x = self.flatten(x)
    #     main_output = self.fc(x)  # Main output
    #     main_probs = tf.nn.softmax(main_output)
    #     else:
    #       return tf.argmax(main_probs, axis=1), 3  # Final output

    def inference(self, input_sample, confidence_threshold=0.8):
      x = self.conv1(tf.expand_dims(input_sample, 0))
      b1 = self.branch1_pool(x)
      b1 = self.branch1_fc(b1)
      b1_probs = tf.nn.softmax(b1)

      if tf.reduce_max(b1_probs) > confidence_threshold:
          return tf.argmax(b1_probs, axis=1)[0], 1  # Early exit at Branch 1

      x = self.conv2(x)
      b2 = self.branch2_pool(x)
      b2 = self.branch2_fc(b2)
      b2_probs = tf.nn.softmax(b2)

      if tf.reduce_max(b2_probs) > confidence_threshold:
          return tf.argmax(b2_probs, axis=1)[0], 2  # Early exit at Branch 2

      x = self.flatten(x)
      main_output = self.fc(x)
      main_probs = tf.nn.softmax(main_output)
      return tf.argmax(main_probs, axis=1)[0], 3  # Final output

# Early Prediction

def early_exit_prediction(inputs, model, confidence_threshold=0.8):
    return model.inference(inputs, confidence_threshold)


loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
optimizer = tf.keras.optimizers.Adam()

# Training

@tf.function
def train_step(inputs, labels, model, optimizer, loss_fn):
    with tf.GradientTape() as tape:
        b1, b2, main_output = model(inputs)
        loss = (loss_fn(labels, b1) +    # Because it's categorical cross entropy, I don't need to compare the softmax(b1) with labels to get the loss. This particular loss fnc takes care of all that. It just needs to be given the output of the fc layer
                loss_fn(labels, b2) +
                loss_fn(labels, main_output))  # Combined loss
    gradients = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(gradients, model.trainable_variables))
    return loss


# Training function
def train_model(model, train_dataset, epochs, optimizer, loss_fn):
    for epoch in range(epochs):
        total_loss = 0
        for inputs, labels in train_dataset:
            loss = train_step(inputs, labels, model, optimizer, loss_fn)
            total_loss += loss.numpy()
        print(f"Epoch {epoch + 1}, Loss: {total_loss:.4f}")


# Evaluation

# def evaluate_with_early_exits(model, dataset, confidence_threshold=0.8):
#     """
#     Evaluates the model with early exits on the given dataset.

#     Args:
#         model: The BranchyNetwork model.
#         dataset: A tf.data.Dataset containing (inputs, labels).
#         confidence_threshold: The confidence threshold for early exits.

#     Returns:
#         A dictionary containing counts and accuracies for early exits and main exits.
#     """
#     total_samples = 0
#     early_exit1_correct = 0
#     early_exit2_correct = 0
#     main_exit_correct = 0

#     early_exit1_count = 0
#     early_exit2_count = 0
#     main_exit_count = 0

#     for inputs, labels in dataset:
#         b1, b2, main_output = model(inputs)

#         # Convert logits to probabilities
#         b1_probs = tf.nn.softmax(b1)
#         b2_probs = tf.nn.softmax(b2)
#         main_probs = tf.nn.softmax(main_output)

#         # Early exit logic

#         # Cast labels to int64 to match predictions data type
#         labels = tf.cast(labels, tf.int64)  # Added line to cast labels

#         if tf.reduce_max(b1_probs) > confidence_threshold:
#             predictions = tf.argmax(b1_probs, axis=1)
#             early_exit1_correct += tf.reduce_sum(tf.cast(predictions == labels, tf.int32))
#             early_exit1_count += len(labels)
#         elif tf.reduce_max(b2_probs) > confidence_threshold:
#             predictions = tf.argmax(b2_probs, axis=1)
#             early_exit2_correct += tf.reduce_sum(tf.cast(predictions == labels, tf.int32))
#             early_exit2_count += len(labels)
#         else:
#             predictions = tf.argmax(main_probs, axis=1)
#             main_exit_correct += tf.reduce_sum(tf.cast(predictions == labels, tf.int32))
#             main_exit_count += len(labels)

#         total_samples += len(labels)

#     # Calculate accuracies
#     # accuracy_early_exit1 = early_exit1_correct / early_exit1_count if early_exit1_count > 0 else 0
#     # accuracy_early_exit2 = early_exit2_correct / early_exit2_count if early_exit2_count > 0 else 0
#     # accuracy_main_exit = main_exit_correct / main_exit_count if main_exit_count > 0 else 0
#     # overall_accuracy = (early_exit1_correct + early_exit2_correct + main_exit_correct) / total_samples

#     # Convert accuracy calculations to TensorFlow tensors before calling .numpy()
#     accuracy_early_exit1 = tf.cast(early_exit1_correct, tf.float32) / tf.cast(early_exit1_count, tf.float32) if early_exit1_count > 0 else tf.constant(0.0)
#     accuracy_early_exit2 = tf.cast(early_exit2_correct, tf.float32) / tf.cast(early_exit2_count, tf.float32) if early_exit2_count > 0 else tf.constant(0.0)
#     accuracy_main_exit = tf.cast(main_exit_correct, tf.float32) / tf.cast(main_exit_count, tf.float32) if main_exit_count > 0 else tf.constant(0.0)
#     overall_accuracy = tf.cast(early_exit1_correct + early_exit2_correct + main_exit_correct, tf.float32) / tf.cast(total_samples, tf.float32)

#     return {
#         "total_samples": total_samples,
#         "early_exit1_count": early_exit1_count,
#         "early_exit2_count": early_exit2_count,
#         "main_exit_count": main_exit_count,
#         "accuracy_early_exit1": accuracy_early_exit1.numpy(),
#         "accuracy_early_exit2": accuracy_early_exit2.numpy(),
#         "accuracy_main_exit": accuracy_main_exit.numpy(),
#         "overall_accuracy": overall_accuracy.numpy(),
#     }

def evaluate_with_early_exits(model, dataset, confidence_threshold=0.8):
    total_samples = 0
    early_exit1_correct = 0
    early_exit2_correct = 0
    main_exit_correct = 0

    early_exit1_count = 0
    early_exit2_count = 0
    main_exit_count = 0

    early_exit1_time = 0
    early_exit2_time = 0
    main_exit_time = 0



    for inputs, labels in dataset:
        # Cast labels to int64 to match predictions data type
        labels = tf.cast(labels, tf.int64)  # Added line to cast labels
        for i in range(len(inputs)):
            start_time = time.time()
            prediction, exit_point = model.inference(inputs[i], confidence_threshold)
            end_time = time.time()
            inference_time = end_time - start_time

            label = labels[i]
            correct = tf.cast(prediction == label, tf.int32)

            if exit_point == 1:
                early_exit1_correct += correct
                early_exit1_count += 1
                early_exit1_time += inference_time
            elif exit_point == 2:
                early_exit2_correct += correct
                early_exit2_count += 1
                early_exit2_time += inference_time
            else:
                main_exit_correct += correct
                main_exit_count += 1
                main_exit_time += inference_time

            total_samples += 1

    # Calculate accuracies
    accuracy_early_exit1 = tf.cast(early_exit1_correct, tf.float32) / tf.cast(early_exit1_count, tf.float32) if early_exit1_count > 0 else tf.constant(0.0)
    accuracy_early_exit2 = tf.cast(early_exit2_correct, tf.float32) / tf.cast(early_exit2_count, tf.float32) if early_exit2_count > 0 else tf.constant(0.0)
    accuracy_main_exit = tf.cast(main_exit_correct, tf.float32) / tf.cast(main_exit_count, tf.float32) if main_exit_count > 0 else tf.constant(0.0)
    overall_accuracy = tf.cast(early_exit1_correct + early_exit2_correct + main_exit_correct, tf.float32) / tf.cast(total_samples, tf.float32)

     # Calculate average inference times
    avg_time_early_exit1 = early_exit1_time / early_exit1_count if early_exit1_count > 0 else 0
    avg_time_early_exit2 = early_exit2_time / early_exit2_count if early_exit2_count > 0 else 0
    avg_time_main_exit = main_exit_time / main_exit_count if main_exit_count > 0 else 0

    return {
        "total_samples": total_samples,
        "early_exit1_count": early_exit1_count,
        "early_exit2_count": early_exit2_count,
        "main_exit_count": main_exit_count,
        "accuracy_early_exit1": accuracy_early_exit1.numpy(),
        "accuracy_early_exit2": accuracy_early_exit2.numpy(),
        "accuracy_main_exit": accuracy_main_exit.numpy(),
        "overall_accuracy": overall_accuracy.numpy(),
        "avg_time_early_exit1": avg_time_early_exit1,
        "avg_time_early_exit2": avg_time_early_exit2,
        "avg_time_main_exit": avg_time_main_exit
    }

# # Example usage:
# # Assuming `test_dataset` is a tf.data.Dataset of (inputs, labels)
# test_dataset = ...  # Replace with your test dataset
# model = BranchyNetwork()  # Replace with your trained BranchyNetwork instance

# # Evaluate with early exits
# evaluation_results = evaluate_with_early_exits(model, test_dataset, confidence_threshold=0.8)
# print("Evaluation Results:")
# for key, value in evaluation_results.items():
#     print(f"{key}: {value}")